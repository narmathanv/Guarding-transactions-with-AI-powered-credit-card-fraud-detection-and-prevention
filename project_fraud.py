# -*- coding: utf-8 -*-
"""Project_Fraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QhRqW-Dj0R6bh6qMQo1UjnwXG-InFKVI
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
# Load the dataset
df = pd.read_csv("/content/creditcard.csv")
# Basic info
print(df.info())
print(df.describe())
print(df.isnull().sum())
# Check for duplicates
duplicates = df.duplicated().sum()
print(f"Duplicate rows: {duplicates}")
# Remove duplicates
df = df.drop_duplicates()
Q1 = df['Amount'].quantile(0.25)
Q3 = df['Amount'].quantile(0.75)
IQR = Q3 - Q1
# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Remove extreme outliers in &#39;Amount&#39;
df = df[(df['Amount'] >= lower_bound) & (df['Amount'] <= upper_bound)]
df['Hour'] = (df['Time'] // 3600) % 24
df.drop(columns='Time', inplace=True)
scaler = StandardScaler()
df[['Amount', 'Hour']] = scaler.fit_transform(df[['Amount','Hour']])
print(df.head())
print(df.describe())

#Distribution of Numerical Features
import matplotlib.pyplot as plt
import seaborn as sns
# Plot histograms for main features
df[['Amount', 'Hour']].hist(bins=30, figsize=(10, 4))
plt.suptitle("Histograms of 'Amount' and 'Hour'")
plt.show()
# Boxplot to detect outliers
sns.boxplot(x=df['Amount'])
plt.title('Boxplot of Transaction Amount')
plt.show()

#Distribution of Target Variable (Class)
sns.countplot(x='Class', data=df)
plt.title('Class Distribution (0 = Legit, 1 = Fraud)')
plt.show()

#Correlation Matrix
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title('Correlation Heatmap')
plt.show()

# Distribution of Amount by Class
sns.histplot(data=df, x='Amount', hue='Class', bins=50, kde=True)
plt.title('Distribution of Amount by Class')
plt.show()
# Hour vs. Class
sns.kdeplot(data=df, x='Hour', hue='Class', fill=True)
plt.title('Distribution of Transaction Hour by Class')
plt.show()

#Pairplot (Selected Features)
selected_features = ['V1','V2', 'V3', 'Amount', 'Class']
sns.pairplot(df[selected_features], hue='Class', corner=True,
plot_kws={'alpha':0.5})
plt.suptitle('Pairplot of Selected Features by Class', y=1.02)
plt.show()

#Train-Test Split
from sklearn.model_selection import train_test_split
# Features and target
X = df.drop(['Class'], axis=1)
y = df['Class']
# Stratified split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42, stratify=y)

#Model 1: Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
# Model training
lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')
lr_model.fit(X_train, y_train)
# Predictions
y_pred_lr = lr_model.predict(X_test)
# Evaluation
print("Logistic Regression:")
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

#Model 2: Random Forest
from sklearn.ensemble import RandomForestClassifier
# Model training
rf_model = RandomForestClassifier(n_estimators=100,
class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)
# Predictions
y_pred_rf = rf_model.predict(X_test)
# Evaluation
print("Random Forest:")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

#Visualization of Results & Model Insights
#(Confusion Matrix)
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Logistic Regression
ConfusionMatrixDisplay.from_estimator(lr_model, X_test, y_test,
display_labels=["Legit", "Fraud"])
plt.title("Confusion Matrix - Logistic Regression")
plt.show()
# Random Forest
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test,
display_labels=["Legit", "Fraud"])
plt.title("Confusion Matrix - Random Forest")
plt.show()

#ROC Curve and AUC Score
from sklearn.metrics import roc_curve, roc_auc_score
# Probabilities
lr_probs = lr_model.predict_proba(X_test)[:, 1]
rf_probs = rf_model.predict_proba(X_test)[:, 1]
# ROC Curves
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')
plt.plot(fpr_rf, tpr_rf, label='Random Forest')
plt.plot([0, 1], [0, 1], linestyle='--', color='grey')
plt.title('ROC Curve Comparison')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#Feature Importance Plot (Random Forest)
import pandas as pd
importances = rf_model.feature_importances_
features = X.columns
# Sort and plot
feat_imp = pd.Series(importances,
index=features).sort_values(ascending=False)[:10]
feat_imp.plot(kind='barh')
plt.title("Top 10 Important Features - Random Forest")
plt.gca().invert_yaxis()
plt.show()

#Visual Comparison of Model Metrics
from sklearn.metrics import precision_score, recall_score, f1_score
metrics = {"Model": ["Logistic Regression", "Random Forest"],"Precision": [precision_score(y_test, y_pred_lr),precision_score(y_test, y_pred_rf)],"Recall": [recall_score(y_test, y_pred_lr),recall_score(y_test, y_pred_rf)],"F1-Score": [f1_score(y_test, y_pred_lr),f1_score(y_test, y_pred_rf)]}
metrics_df = pd.DataFrame(metrics)
metrics_df.set_index("Model").plot(kind="bar", figsize=(8, 6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.ylim(0, 1.1)
plt.legend(loc='lower right')
plt.xticks(rotation=0)
plt.show()